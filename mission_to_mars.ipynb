{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "from splinter import Browser\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which chromedriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Splinter to read html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract From NASA Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "url = 'https://mars.nasa.gov/news'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read html from website\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to store text from website\n",
    "top_article_title =\"\"\n",
    "top_article_body = \"\"\n",
    "nasa_news = {} # Using this variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store html in var\n",
    "html = browser.html\n",
    "\n",
    "# Read html tags into var using BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "titles = soup.find('div', class_=\"list_text\")\n",
    "top_article_title = titles.find('a').text\n",
    "top_article_body = titles.find('div',class_=\"article_teaser_body\").text\n",
    "\n",
    "# traverse through the dictionary using [] and use = to set value as \n",
    "nasa_news[top_article_title] = top_article_body\n",
    "\n",
    "\n",
    "print(nasa_news)\n",
    "# for each_title in titles:\n",
    "#     print(each_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract from JPL Featured Space Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jpl_url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.visit(jpl_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to store text from website\n",
    "feat_img_url = \"\" \n",
    "feat_img_url_1 = \"\"\n",
    "feat_img_url_2 = \"\" #Using This one to retrieve url\n",
    "feat_img_url_new = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find image parth first part\n",
    "find_div_1 = soup.find('div', class_=\"jpl_logo\")\n",
    "feat_img_url_1 = find_div_1.a['href']\n",
    "\n",
    "feat_img_url_1='http:'+feat_img_url_1\n",
    "feat_img_url_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find image parth first part\n",
    "find_div_raw = soup.find('li', class_=\"slide\")\n",
    "feat_img_url = find_div_raw.a['data-fancybox-href']\n",
    "feat_img_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get url for the mars latest image\n",
    "feat_img_url_2=feat_img_url_1+feat_img_url[1:]\n",
    "feat_img_url_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Mars Weather from Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_weather = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_url = 'https://twitter.com/marswxreport?lang=en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "browser.visit(twitter_url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Mars Weather from attributes\n",
    "extract_mars_weather = soup.find_all('div', attrs={'data-testid': 'tweet'})\n",
    "for each_weather in extract_mars_weather: \n",
    "    mars_weather_info = each_weather.find_all('span')[4].text\n",
    "#     print(mars_weather)\n",
    "    break\n",
    "\n",
    "mars_weather = {\"mars_weather\":mars_weather_info}\n",
    "print(mars_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Mar Facts from Space Facts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://space-facts.com/mars/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable to contain mars facts data frame\n",
    "mars_facts = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Var to store Mars info\n",
    "info_dict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from website\n",
    "tables = pd.read_html(url)\n",
    "type(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assign table into dictionary \"info_dict{}\"\n",
    "target_table=tables[0]\n",
    "\n",
    "for each_row in target_table.index:\n",
    "    each_row_df=target_table.loc[each_row]\n",
    "    info_dict[each_row_df[0]]=each_row_df[1]\n",
    "\n",
    "print(info_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Mars Hemispheres from USGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "usgs_url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to contain images and title of the images \n",
    "img_url = \"\"\n",
    "title = \"\"\n",
    "hemispheres_image_urls = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.visit(usgs_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "titles = soup.find_all('div', class_=\"item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(titles)\n",
    "for each_title in titles: \n",
    "    print('for')\n",
    "    key_word = each_title.find('h3').text\n",
    "    print(key_word)\n",
    "#     browser.find_link_by_partial_text('Hemisphere Enhanced').click()\n",
    "    browser.find_link_by_partial_text(key_word).click()\n",
    "#     print(browser.url)\n",
    "    # you can access the attribute using [href]\n",
    "    img_url = browser.find_link_by_text('Sample').first['href']\n",
    "#     hemisphere_image_urls.update({test.text:img_url})\n",
    "    print(img_url)\n",
    "    hemisphere_image_urls[key_word]=img_url\n",
    "    browser.back()\n",
    "# print(titles.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: a\n",
      "1: b\n",
      "2: c\n"
     ]
    }
   ],
   "source": [
    "titles=['a', 'b', 'c']\n",
    "for index, title in enumerate(titles):\n",
    "    print(f'{index}: {title}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cerberus Hemisphere Enhanced\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JeabD\\Anaconda3\\lib\\site-packages\\splinter\\driver\\webdriver\\__init__.py:528: FutureWarning: browser.find_link_by_partial_text is deprecated. Use browser.links.find_by_partial_text instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/cerberus_enhanced.tif/full.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JeabD\\Anaconda3\\lib\\site-packages\\splinter\\driver\\webdriver\\__init__.py:536: FutureWarning: browser.find_link_by_text is deprecated. Use browser.links.find_by_text instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-c282bcb225de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mimage_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'img_url'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg_url\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mhemispheres_image_urls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "for index, each_title in enumerate(titles): \n",
    "        key_word = each_title.find('h3').text\n",
    "        print(key_word)\n",
    "        browser.find_link_by_partial_text(key_word).click()\n",
    "        img_url = browser.find_link_by_text('Sample').first['href']\n",
    "        print(img_url)\n",
    "        \n",
    "        image_dict = {}\n",
    "        image_dict['title'] = key_word\n",
    "        image_dict['img_url'] = img_url\n",
    "    \n",
    "        hemispheres_image_urls[index](image_dict)\n",
    "        # hemispheres_image_urls={\n",
    "        #    \"1\": {key_word: img_url}, \n",
    "        # }\n",
    "        browser.back()\n",
    "print(f\"hemis dict = {hemispheres_image_urls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemispheres_image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
